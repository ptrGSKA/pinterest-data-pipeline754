{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3514e8b-f97e-4187-8b21-07d3b6f83ba5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# AWS Kinesis data streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7837f57c-7b93-48e0-8a6d-621a57b293f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Preparation for streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fec6da7-feb5-4605-8f47-3f1062e43678",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Path to the AWS credentials.\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# AWS access key and secret key.\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "# Names of the three streams from Kinesis\n",
    "pin_stream = 'streaming-12a740a19697-pin'\n",
    "geo_stream = 'streaming-12a740a19697-geo'\n",
    "user_stream = 'streaming-12a740a19697-user'\n",
    "\n",
    "# Names of the three delta table to write stream to\n",
    "pin_table = '12a740a19697_pin_table'\n",
    "geo_table = '12a740a19697_geo_table'\n",
    "user_table = '12a740a19697_user_table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4b84b0-75b6-4b06-8f73-57a1a64cf369",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.formatCheck.enabled</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.delta.formatCheck.enabled",
         "false"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "388ca952-1b2a-4975-ab16-e44029f412da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5412d6-ca68-4caa-bc1f-417bd6e9dab0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_kinesis_stream(stream_name, stream_schema):\n",
    "    '''\n",
    "    This function reads stream data from an AWS Kinesis,\n",
    "    deserialises and by using the given schema casting\n",
    "    it into a table format.\n",
    "\n",
    "    Args:\n",
    "        stream_name (string): name of the Kinesis stream to read from.\n",
    "        stream_schema (StructType): StructType object that contains the \n",
    "                                    casting format.\n",
    "    Returns:\n",
    "        The deserialized and casted table.\n",
    "    '''\n",
    "    \n",
    "    kinesis_stream = spark \\\n",
    "                    .readStream \\\n",
    "                    .format('kinesis') \\\n",
    "                    .option('streamName', stream_name) \\\n",
    "                    .option('initialPosition','earliest') \\\n",
    "                    .option('region','us-east-1') \\\n",
    "                    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "                    .option('awsSecretKey', SECRET_KEY) \\\n",
    "                    .option('inferSchema', 'True') \\\n",
    "                    .load()\n",
    "\n",
    "    # Deserializing the stream data and casting it into a table\n",
    "    df = kinesis_stream.selectExpr(\"CAST(data as STRING)\") \\\n",
    "                       .withColumn(\"data\", from_json(col(\"data\"), stream_schema)) \\\n",
    "                       .select(col(\"data.*\"))\n",
    "    return df\n",
    "\n",
    "def write_stream_to_delta_table(df, table_name):\n",
    "    '''\n",
    "    This function writes stream into databricks delta tables.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): the table to save\n",
    "        table_name (string): the name of the table to save the data to\n",
    "    \n",
    "    Returns:\n",
    "        None, creates a delta table.\n",
    "    '''\n",
    "    \n",
    "    df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", f\"/tmp/kinesis/{table_name}_checkpoints/\") \\\n",
    "        .table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "173d10e8-1bcc-423c-a974-cf36c7ae7644",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b76098c8-817c-4fa2-bbe8-dd5e1b8ea4c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Pinterest data ingestion, cleaning and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc2f6d6b-80d1-40ec-8df7-2f31d302328b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the schema for the pinterest stream\n",
    "pin_stream_schema = StructType([\n",
    "        StructField('index', IntegerType()),\n",
    "        StructField('unique_id', StringType()),\n",
    "        StructField('title', StringType()),\n",
    "        StructField('description', StringType()),\n",
    "        StructField('poster_name', StringType()),\n",
    "        StructField('follower_count', IntegerType()),\n",
    "        StructField('tag_list', StringType()),\n",
    "        StructField('is_image_or_video', StringType()),\n",
    "        StructField('image_src', StringType()),\n",
    "        StructField('save_location', StringType()),\n",
    "        StructField('category', StringType())\n",
    "        ])\n",
    "\n",
    "# Ingesting the pinterest stream data\n",
    "df_pin = read_kinesis_stream(pin_stream, pin_stream_schema)\n",
    "\n",
    "# Dictionary containing the value and column to clean.\n",
    "# Key-Value pair reversed due to the dictionary key restriction.\n",
    "to_replace = {\n",
    "    'No description available': 'description',\n",
    "    'Image src error.': 'image_src',\n",
    "    'User Info Error': 'poster_name',\n",
    "    'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e': 'tag_list', \n",
    "    'No Title Data Available': 'title',\n",
    "    'No description available Story format': 'description',\n",
    "    'Untitled': 'description'\n",
    "}\n",
    "\n",
    "# Looping through the dictionary and calling the replace function to remove unnecessary data.\n",
    "for value, column in to_replace.items():\n",
    "    df_pin = df_pin.replace({value: None}, subset=[column])\n",
    "\n",
    "# There is an extra cleaning necessary as it would be a duplicate key entry in the dictionary.\n",
    "df_pin = df_pin.replace({'User Info Error': None}, subset=['follower_count'])\n",
    "\n",
    "# Dropping duplicate rows.\n",
    "df_pin = df_pin.dropDuplicates(df_pin.columns)\n",
    "\n",
    "# Rewriting the follower_count column to be numeric only.\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace(df_pin['follower_count'], 'k','000'))\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace(df_pin['follower_count'], 'M','000000'))\n",
    "\n",
    "# Transforming the column to integer format.\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin['follower_count'].cast('int'))\n",
    "df_pin = df_pin.withColumn('index', df_pin['index'].cast('int'))\n",
    "\n",
    "# Cleaning the save_location column\n",
    "df_pin = df_pin.withColumn('save_location', regexp_replace(df_pin['save_location'], 'Local save in ',''))\n",
    "\n",
    "# Renaming index column\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "\n",
    "# Rearranging the columns\n",
    "df_pin = df_pin.select('ind',\n",
    "                    'unique_id',\n",
    "                    'title',\n",
    "                    'description',\n",
    "                    'follower_count',\n",
    "                    'poster_name',\n",
    "                    'tag_list',\n",
    "                    'is_image_or_video',\n",
    "                    'image_src',\n",
    "                    'save_location',\n",
    "                    'category'\n",
    ")\n",
    "\n",
    "# Function call to write data into delta table\n",
    "write_stream_to_delta_table(df_pin, pin_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4410d4e-60a1-4a52-8ee5-8019a0ebc017",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Geolocation data ingestion, cleaning and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17445a78-7335-42e9-b93d-e64180192593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the schema for the geolocation stream\n",
    "geo_stream_schema = StructType([\n",
    "        StructField('ind', IntegerType()),\n",
    "        StructField('country', StringType()),\n",
    "        StructField('latitude', StringType()),\n",
    "        StructField('longitude', StringType()),\n",
    "        StructField('timestamp', TimestampType())\n",
    "    ])\n",
    "\n",
    "# Ingesting the geolocation stream data\n",
    "df_geo = read_kinesis_stream(geo_stream, geo_stream_schema)\n",
    "\n",
    "# Creating a new column coordinates with the latitude and longitude as array.\n",
    "df_geo = df_geo.withColumn('coordinates', array(df_geo['latitude'], df_geo['longitude']))\n",
    "\n",
    "# Dropping the latutide and longitude columns as no longer needed.\n",
    "df_geo = df_geo.drop('latitude','longitude')\n",
    "\n",
    "# Casting the timestamp column into timestamp format.\n",
    "df_geo = df_geo.withColumn('timestamp', to_timestamp('timestamp'))\n",
    "df_geo = df_geo.withColumn('timestamp', date_format('timestamp', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Dropping duplicate entries\n",
    "df_pin = df_pin.dropDuplicates(df_pin.columns)\n",
    "\n",
    "# Reordering the columns\n",
    "df_geo = df_geo.select('ind',\n",
    "                       'country',\n",
    "                       'coordinates',\n",
    "                       'timestamp')\n",
    "\n",
    "# Function call to write data into delta table\n",
    "write_stream_to_delta_table(df_geo, geo_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5b96e9d-c83a-418c-a929-4b971fbbf054",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Users data ingestion, cleaning and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83523404-d487-4708-a576-124f030c4339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the schema for the user stream\n",
    "user_stream_schema = StructType([\n",
    "        StructField('ind', IntegerType()),\n",
    "        StructField('first_name', StringType()),\n",
    "        StructField('last_name', StringType()),\n",
    "        StructField('age', IntegerType()),\n",
    "        StructField('date_joined', TimestampType())\n",
    "    ])\n",
    "\n",
    "# Ingesting the user stream data\n",
    "df_user = read_kinesis_stream(user_stream, user_stream_schema)\n",
    "\n",
    "# Creating a new colum containing the first name and last name of the user\n",
    "df_user = df_user.withColumn('user_name', concat(df_user['first_name'], lit(' '), df_user['last_name']))\n",
    "\n",
    "# Dropping the first_name and last_name columns as no longer needed.\n",
    "df_user = df_user.drop('first_name','last_name')\n",
    "\n",
    "# Casting the date_joined column into timestamp format.\n",
    "df_user = df_user.withColumn('date_joined', to_timestamp('date_joined'))\n",
    "df_user = df_user.withColumn('date_joined', date_format('date_joined', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Dropping duplicate entries\n",
    "df_pin = df_pin.dropDuplicates(df_pin.columns)\n",
    "\n",
    "# Reordering the columns\n",
    "df_user = df_user.select('ind',\n",
    "                         'user_name',\n",
    "                         'age',\n",
    "                         'date_joined')\n",
    "\n",
    "# Function call to write data into delta table\n",
    "write_stream_to_delta_table(df_user, user_table)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2803508969911762,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AWS_Kinesis_data_streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
